{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f51c2f1f-c9eb-46bd-87a0-bfed13821997",
   "metadata": {
    "executionInfo": {
     "elapsed": 6671,
     "status": "ok",
     "timestamp": 1748006552026,
     "user": {
      "displayName": "JORDI GUILL√âN GONZ√ÅLEZ",
      "userId": "15811336653197258391"
     },
     "user_tz": -120
    },
    "id": "f51c2f1f-c9eb-46bd-87a0-bfed13821997"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3eb8a-3bbd-4c8e-8232-69ebe41d045c",
   "metadata": {
    "id": "4ac3eb8a-3bbd-4c8e-8232-69ebe41d045c"
   },
   "source": [
    "# Seminar 5 - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb921ac-6386-4074-81ae-18756059b2ae",
   "metadata": {},
   "source": [
    "Javier Gonz√°lez Otero - 243078\n",
    "\n",
    "Jordi Guill√©n Gonz√°lez - 253027\n",
    "\n",
    "David S√°nchez Maldonado - 253798"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119a918-efd1-4f7a-92c9-7e1c5a97a10e",
   "metadata": {
    "id": "1119a918-efd1-4f7a-92c9-7e1c5a97a10e"
   },
   "source": [
    "## Part 0: Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe68498-e2f6-4112-92db-fcdcd358e294",
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1748009659904,
     "user": {
      "displayName": "JORDI GUILL√âN GONZ√ÅLEZ",
      "userId": "15811336653197258391"
     },
     "user_tz": -120
    },
    "id": "fbe68498-e2f6-4112-92db-fcdcd358e294"
   },
   "outputs": [],
   "source": [
    "file_path_clients = \"data/client_datasets/\"  # Path to the directory containing training data for each client\n",
    "\n",
    "# Lists to store training features and labels DataFrames for each client\n",
    "train_features_dfs = []\n",
    "train_labels_dfs = []\n",
    "\n",
    "# Loop over the 10 clients\n",
    "for client_id in range(1, 11):\n",
    "    # Load training features and labels for the current client\n",
    "    features_df = pd.read_csv(f\"{file_path_clients}client_{client_id}_features.csv\", header=None)\n",
    "    labels_df = pd.read_csv(f\"{file_path_clients}client_{client_id}_labels.csv\", header=None)\n",
    "\n",
    "    # Append the loaded data to the lists\n",
    "    train_features_dfs.append(features_df)\n",
    "    train_labels_dfs.append(labels_df)\n",
    "\n",
    "# Load test features and labels\n",
    "test_features_df = pd.read_csv('data/test_features.csv', header=None)\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22246541-b81d-4ba7-82e6-12e4690578d2",
   "metadata": {},
   "source": [
    "**Check for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6352e7b-165a-4327-85b0-dddf80a64227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Checks for missing (NaN) values in a DataFrame.\n",
    "\n",
    "    Prints the total number of missing values and \n",
    "    the row indices where they occur, if any.\n",
    "    \"\"\"\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        print(f\"Missing values found: {total_missing}\")\n",
    "        missing_rows = df[df.isnull().any(axis=1)]\n",
    "        print(f\"Rows with missing values:\\n{missing_rows.index.tolist()}\")\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2ebb38-aae4-4782-9502-1e4a85392d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training data (features and labels):\n",
      "\n",
      "Client 1\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 2\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 3\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 4\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 5\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 6\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 7\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 8\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 9\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Client 10\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n",
      "\n",
      "Checking test data (features and labels):\n",
      "- Features:\n",
      "No missing values found.\n",
      "- Labels:\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking training data (features and labels):\")\n",
    "for i in range(10):  # 10 clients\n",
    "    print(f\"\\nClient {i+1}\")\n",
    "    print(\"- Features:\")\n",
    "    check_missing_values(train_features_dfs[i])\n",
    "    print(\"- Labels:\")\n",
    "    check_missing_values(train_labels_dfs[i])\n",
    "print(\"\\nChecking test data (features and labels):\")\n",
    "print(\"- Features:\")\n",
    "check_missing_values(test_features_df)\n",
    "print(\"- Labels:\")\n",
    "check_missing_values(test_labels_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0e744-56ff-482d-a50c-5b813ac82178",
   "metadata": {},
   "source": [
    "No cleaning is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ec3ce-3668-4b19-8b43-50c2168187cc",
   "metadata": {},
   "source": [
    "**Obtain the data proportion associated to each client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ad4fc6-846e-4fb0-8143-69dc4fd45150",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_client_1 = len(train_features_dfs[0])\n",
    "num_samples_client_2 = len(train_features_dfs[1])\n",
    "num_samples_client_3 = len(train_features_dfs[2])\n",
    "num_samples_client_4 = len(train_features_dfs[3])\n",
    "num_samples_client_5 = len(train_features_dfs[4])\n",
    "num_samples_client_6 = len(train_features_dfs[5])\n",
    "num_samples_client_7 = len(train_features_dfs[6])\n",
    "num_samples_client_8 = len(train_features_dfs[7])\n",
    "num_samples_client_9 = len(train_features_dfs[8])\n",
    "num_samples_client_10 = len(train_features_dfs[9])\n",
    "\n",
    "total_samples = (\n",
    "    num_samples_client_1 + num_samples_client_2 + num_samples_client_3 +\n",
    "    num_samples_client_4 + num_samples_client_5 + num_samples_client_6 +\n",
    "    num_samples_client_7 + num_samples_client_8 + num_samples_client_9 +\n",
    "    num_samples_client_10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596381c-de94-4cba-9e7c-6bc6ec7e6f8e",
   "metadata": {
    "id": "c596381c-de94-4cba-9e7c-6bc6ec7e6f8e"
   },
   "source": [
    "## Part 1 - ML model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Ms927kCTiyP",
   "metadata": {
    "id": "_Ms927kCTiyP"
   },
   "source": [
    "De momenento uso el svm del semi 2 que ha dicho que lo podemos usar. Se puede cambiar al que queramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "VBGvGS3cR4De",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1748006674654,
     "user": {
      "displayName": "JORDI GUILL√âN GONZ√ÅLEZ",
      "userId": "15811336653197258391"
     },
     "user_tz": -120
    },
    "id": "VBGvGS3cR4De"
   },
   "outputs": [],
   "source": [
    "def train_svm_model(X_train, y_train, kernel='rbf', C=1.0, gamma='scale'):\n",
    "    '''\n",
    "    Trains a Support Vector Machine (SVM) classifier on the given CSI feature data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame or np.ndarray): CSI features (shape: [n_samples, 270]).\n",
    "    - y_train (pd.Series or np.ndarray): Labels (values from 1 to 5).\n",
    "    - kernel (str): Kernel type ('linear', 'rbf', 'poly', etc.)\n",
    "    - C (float): Regularization parameter.\n",
    "    - gamma (str or float): Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained SVM model.\n",
    "    - scaler: StandardScaler used for normalization.\n",
    "    '''\n",
    "    # flatten train labels\n",
    "    y_train = y_train.iloc[0, :].values.ravel()\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Train SVM\n",
    "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "    svm.fit(X_scaled, y_train)\n",
    "\n",
    "    return svm, scaler\n",
    "\n",
    "def predict_with_svm(model, scaler, X_test):\n",
    "    '''\n",
    "    Generates predictions on the test dataset using a trained SVM model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained SVM classifier.\n",
    "    - scaler: StandardScaler used during training.\n",
    "    - X_test (pd.DataFrame or np.ndarray): Test features.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Predicted labels for the test samples.\n",
    "    '''\n",
    "    X_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0c6d2-499d-4fc9-8866-d6a655e11f41",
   "metadata": {},
   "source": [
    "A FC netwok is selected as ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc3ee6a-a1b7-49c5-b301-5f308273eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=270, hidden_dim=128, output_dim=12):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # logits (use CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6lnOAvTSQuaT",
   "metadata": {
    "id": "6lnOAvTSQuaT"
   },
   "source": [
    "## Part 2 - Preparation of the FL setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c52d4d-483d-4d6c-82e3-de0532d6fdd6",
   "metadata": {},
   "source": [
    "To perform a realistic simulation of the FL environment, clients must be instantiated separately from orchestrator, hence, we create an independent class for treatig their data and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f054431-2494-44af-8e04-d390cdc5eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, X_train, y_train, local_epochs=5, local_batch_size=32, local_lr=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a client with its local training data and training hyperparameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_id : int\n",
    "            Identifier for the client.\n",
    "        X_train : pd.DataFrame\n",
    "            Feature matrix.\n",
    "        y_train : pd.DataFrame\n",
    "            Labels (in a single row or column).\n",
    "        local_epochs : int\n",
    "            Number of local epochs for training.\n",
    "        local_batch_size : int\n",
    "            Batch size for local training.\n",
    "        local_lr : float\n",
    "            Learning rate for local optimizer.\n",
    "        \"\"\"\n",
    "        self.id = client_id\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.local_epochs = local_epochs\n",
    "        self.local_batch_size = local_batch_size\n",
    "        self.local_lr = local_lr\n",
    "\n",
    "    def local_train(self, global_model):\n",
    "        \"\"\"\n",
    "        Perform local training using the provided global model as a starting point.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        global_model : torch.nn.Module\n",
    "            The global model whose weights are used as the starting point.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        state_dict : dict\n",
    "            The updated model weights after local training.\n",
    "        num_samples : int\n",
    "            The number of training samples used.\n",
    "        \"\"\"\n",
    "        model = type(global_model)()  # create a new instance of the model class\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.local_lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        model.train()\n",
    "        X_tensor = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y_train.values.ravel() - 1, dtype=torch.long) # Substract 1 since torch receives [0 - num_classes]\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.local_batch_size, shuffle=True)\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return model.state_dict(), len(self.X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbb594-fce5-4f59-8d83-5fbcbe32028e",
   "metadata": {},
   "source": [
    "Federated AVG is selected as aggregation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1335b15f-3657-4dde-ae98-26b6afebaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(weight_updates, sizes):\n",
    "    \"\"\"\n",
    "    Performs Federated Averaging over model weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weight_updates : list of state_dicts (models' weights)\n",
    "    sizes : list of int, number of samples per client\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_weights : state_dict representing the averaged model\n",
    "    \"\"\"\n",
    "    total_size = sum(sizes)\n",
    "    avg_weights = {}\n",
    "\n",
    "    for key in weight_updates[0].keys():\n",
    "        avg_weights[key] = sum(\n",
    "            update[key] * (size / total_size)\n",
    "            for update, size in zip(weight_updates, sizes)\n",
    "        )\n",
    "\n",
    "    return avg_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb10b9f7-a9d3-43bc-849e-f558659afe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    def __init__(self, clients, model_class, num_fl_rounds, tolerance_fl, input_dim=270, hidden_dim=128, output_dim=12):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        clients : list of Client\n",
    "            List of clients that contain and manage their own data.\n",
    "        model_class : callable\n",
    "            The PyTorch model class to instantiate the global model.\n",
    "        num_fl_rounds : int\n",
    "            Number of global federated training rounds.\n",
    "        tolerance_fl : float\n",
    "            Minimum weight change threshold to consider convergence.\n",
    "        input_dim, hidden_dim, output_dim : int\n",
    "            Architecture parameters for the model.\n",
    "        \"\"\"\n",
    "        self.clients = clients\n",
    "        self.model_class = model_class\n",
    "        self.num_fl_rounds = num_fl_rounds\n",
    "        self.tolerance_fl = tolerance_fl\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.global_model = None\n",
    "\n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initializes a fresh instance of the global model.\"\"\"\n",
    "        self.global_model = self.model_class(\n",
    "            input_dim=self.input_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.output_dim\n",
    "        )\n",
    "\n",
    "    def select_clients(self, num_clients):\n",
    "        \"\"\"Randomly selects a subset of clients for a given round.\"\"\"\n",
    "        return random.sample(self.clients, num_clients)\n",
    "\n",
    "    def run_federated_training(self, fedavg_fn, clients_per_round=None):\n",
    "        \"\"\"\n",
    "        Runs the Federated Learning process over several rounds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fedavg_fn : function\n",
    "            Function that aggregates model weights using FedAvg.\n",
    "        clients_per_round : int or None\n",
    "            Number of clients per round (defaults to using all).\n",
    "        \"\"\"\n",
    "        if clients_per_round is None:\n",
    "            clients_per_round = len(self.clients)\n",
    "\n",
    "        prev_weights = None\n",
    "\n",
    "        for round_num in range(self.num_fl_rounds):\n",
    "            print(f\"\\n[Round {round_num + 1}] Selecting clients...\")\n",
    "            selected_clients = self.select_clients(clients_per_round)\n",
    "\n",
    "            print(f\"[Round {round_num + 1}] Training local models...\")\n",
    "            updates = []\n",
    "            sizes = []\n",
    "            for client in selected_clients:\n",
    "                weights, size = client.local_train(self.global_model)\n",
    "                updates.append(weights)\n",
    "                sizes.append(size)\n",
    "\n",
    "            print(f\"[Round {round_num + 1}] Aggregating with FedAvg...\")\n",
    "            avg_weights = fedavg_fn(updates, sizes)\n",
    "            self.global_model.load_state_dict(avg_weights)\n",
    "\n",
    "            if prev_weights is not None:\n",
    "                deltas = sum(torch.norm(avg_weights[k] - prev_weights[k]).item() for k in avg_weights)\n",
    "                print(f\"[Round {round_num + 1}] Weight change: {deltas:.6f}\")\n",
    "                if deltas < self.tolerance_fl:\n",
    "                    print(\"[Convergence reached]\")\n",
    "                    break\n",
    "\n",
    "            prev_weights = {k: v.clone().detach() for k, v in avg_weights.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1qrRF3Z6Q0Vh",
   "metadata": {
    "id": "1qrRF3Z6Q0Vh"
   },
   "source": [
    "## Part 3 - Collaborative training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e63663da-9e30-4cf4-9646-8029540f9910",
   "metadata": {
    "id": "KpIwH78PQ7LM"
   },
   "outputs": [],
   "source": [
    "# Global FL hyperparameters\n",
    "NUM_FL_ROUNDS = 10\n",
    "TOLERANCE_FL = 1e-3\n",
    "CLIENTS_PER_ROUND = 3\n",
    "\n",
    "# Local training hyperparameters per client\n",
    "LOCAL_EPOCHS = 5\n",
    "LOCAL_BATCH_SIZE = 32\n",
    "LOCAL_LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "714beb57-fd96-49d6-9d02-f67670d93da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = []\n",
    "\n",
    "for i in range(10):\n",
    "    client = Client(\n",
    "        client_id=i + 1,\n",
    "        X_train=train_features_dfs[i],\n",
    "        y_train=train_labels_dfs[i],\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        local_batch_size=LOCAL_BATCH_SIZE,\n",
    "        local_lr=LOCAL_LR\n",
    "    )\n",
    "    clients.append(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5658d6d7-9729-44be-a84f-80ab490a9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the central FL orchestrator\n",
    "orchestrator = Orchestrator(\n",
    "    clients=clients,\n",
    "    model_class=PoseClassifier,\n",
    "    num_fl_rounds=NUM_FL_ROUNDS,\n",
    "    tolerance_fl=TOLERANCE_FL,\n",
    "    input_dim=270,\n",
    "    hidden_dim=128,\n",
    "    output_dim=12\n",
    ")\n",
    "\n",
    "# Instantiate the global model\n",
    "orchestrator.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6d9059d-41f2-4c47-b22d-f19beb35d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Round 1] Selecting clients...\n",
      "[Round 1] Training local models...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 12 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start federated training across rounds\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m orchestrator\u001b[38;5;241m.\u001b[39mrun_federated_training(\n\u001b[0;32m      3\u001b[0m     fedavg_fn\u001b[38;5;241m=\u001b[39mfed_avg,\n\u001b[0;32m      4\u001b[0m     clients_per_round\u001b[38;5;241m=\u001b[39mCLIENTS_PER_ROUND\n\u001b[0;32m      5\u001b[0m )\n",
      "Cell \u001b[1;32mIn[31], line 62\u001b[0m, in \u001b[0;36mOrchestrator.run_federated_training\u001b[1;34m(self, fedavg_fn, clients_per_round)\u001b[0m\n\u001b[0;32m     60\u001b[0m sizes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m selected_clients:\n\u001b[1;32m---> 62\u001b[0m     weights, size \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mlocal_train(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_model)\n\u001b[0;32m     63\u001b[0m     updates\u001b[38;5;241m.\u001b[39mappend(weights)\n\u001b[0;32m     64\u001b[0m     sizes\u001b[38;5;241m.\u001b[39mappend(size)\n",
      "Cell \u001b[1;32mIn[27], line 60\u001b[0m, in \u001b[0;36mClient.local_train\u001b[1;34m(self, global_model)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(model(xb), yb)\n\u001b[0;32m     61\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1297\u001b[0m         target,\n\u001b[0;32m   1298\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1299\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1300\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1301\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1302\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3495\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3496\u001b[0m     target,\n\u001b[0;32m   3497\u001b[0m     weight,\n\u001b[0;32m   3498\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3499\u001b[0m     ignore_index,\n\u001b[0;32m   3500\u001b[0m     label_smoothing,\n\u001b[0;32m   3501\u001b[0m )\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 12 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Start federated training across rounds\n",
    "orchestrator.run_federated_training(\n",
    "    fedavg_fn=fed_avg,\n",
    "    clients_per_round=CLIENTS_PER_ROUND\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8eb8-5604-417f-896d-20ade0ab12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set from CSV\n",
    "test_features_df = pd.read_csv('data/test_features.csv', header=None)\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e050d4-e795-47bf-b5b4-1e81950ccbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test_df, y_test_df):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained global model.\n",
    "    X_test_df : pd.DataFrame\n",
    "        Feature matrix of the test set.\n",
    "    y_test_df : pd.DataFrame\n",
    "        True labels of the test set.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    Prints accuracy and confusion matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_tensor = torch.tensor(X_test_df.values, dtype=torch.float32)\n",
    "    y_true = y_test_df.values.ravel()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        y_pred = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nüß™ Final Test Accuracy: {acc:.4f}\")\n",
    "    print(\"üìä Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbd500-4f39-432f-a77d-b3757b293aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluate_model(orchestrator.global_model, test_features_df, test_labels_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
